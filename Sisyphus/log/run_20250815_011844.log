2025-08-15 01:18:44,506 INFO: Waiting for Ollama to be ready...
2025-08-15 01:21:15,026 INFO: Ollama did not start in time.
2025-08-15 01:21:15,026 INFO: Ollama server is not running. Starting Ollama.
2025-08-15 01:21:15,035 INFO: Starting Ollama server...
2025-08-15 01:21:15,035 INFO: Waiting for Ollama to be ready...
2025-08-15 01:21:17,087 INFO: Ollama is running.
2025-08-15 01:21:17,573 INFO: Refreshing options...
2025-08-15 01:21:19,634 INFO: Options refreshed:
2025-08-15 01:21:19,634 INFO: Models: ['deepseek-r1:8b', 'llama3:8b']
2025-08-15 01:21:19,634 INFO: Systems: ['system1.txt', 'system_consistency.txt', 'system_consistency_cl.txt', 'system_cover_letter.txt']
2025-08-15 01:21:19,635 INFO: CVs: ['cv1.txt']
2025-08-15 01:21:19,635 INFO: CV Templates: ['cv_template.docx', 'cv_template2.docx', 'cv_template21.docx', 'cv_template21_simplest.docx']
2025-08-15 01:21:19,635 INFO: CL Templates: ['cl_template.docx']
2025-08-15 01:21:19,635 INFO: Previously Saved CV Outputs: ['cltrials.txt', 'consistency_tests0.txt', 'out_cv_20250724_213613.txt', 'out_cv_20250725_000423.txt', 'out_cv_20250725_001236.txt', 'out_cv_20250725_003414.txt', 'out_cv_20250725_004330.txt', 'out_cv_20250725_005646.txt', 'out_cv_20250730_011841.txt', 'out_cv_20250730_020338.txt', 'stableish0.txt']
2025-08-15 01:21:19,635 INFO: Previously Saved CL Outputs: ['out_cl_20250722_234433.txt', 'out_cl_20250723_230452.txt']
2025-08-15 01:21:35,445 INFO: CV loaded from C:\CodeProjects\Sisyphus\Sisyphus\saved_outputs\consistency_tests0.txt
2025-08-15 01:21:38,140 INFO: Job description is empty. Please enter a job description.
2025-08-15 01:21:47,742 INFO: JS Output: 97

2025-08-15 01:21:47,742 INFO: [MODEL: llama3:8b] Input token usage: 2.37%
2025-08-15 01:21:47,742 INFO: [MODEL: llama3:8b] Input uses 97 tokens, remaining for response: 3999.
2025-08-15 01:22:09,043 INFO: JS Output: 214

2025-08-15 01:22:09,043 INFO: [MODEL: llama3:8b] Output token usage: 5.22%
2025-08-15 01:22:09,044 INFO: [MODEL: llama3:8b] Output uses 214 tokens, remaining for response: 3785.
2025-08-15 01:22:09,045 INFO: slide_summary: candidate_name: Jane Doe
2025-08-15 01:22:09,045 INFO: slide_summary: candidate_title: Senior Software Engineer
2025-08-15 01:22:09,045 INFO: slide_summary: general_txts: 4
2025-08-15 01:22:09,045 INFO: slide_summary: special_txts: 6
2025-08-15 01:22:09,508 INFO: JS Output: 241

2025-08-15 01:22:09,509 INFO: [MODEL: llama3:8b] Input token usage: 5.88%
2025-08-15 01:22:09,509 INFO: [MODEL: llama3:8b] Input uses 241 tokens, remaining for response: 3855.
2025-08-15 01:22:20,485 INFO: JS Output: 137

2025-08-15 01:22:20,486 INFO: [MODEL: llama3:8b] Output token usage: 3.34%
2025-08-15 01:22:20,486 INFO: [MODEL: llama3:8b] Output uses 137 tokens, remaining for response: 3718.
2025-08-15 01:22:20,963 INFO: JS Output: 191

2025-08-15 01:22:20,964 INFO: [MODEL: llama3:8b] Input token usage: 4.66%
2025-08-15 01:22:20,964 INFO: [MODEL: llama3:8b] Input uses 191 tokens, remaining for response: 3905.
2025-08-15 01:22:29,407 INFO: JS Output: 97

2025-08-15 01:22:29,408 INFO: [MODEL: llama3:8b] Output token usage: 2.37%
2025-08-15 01:22:29,408 INFO: [MODEL: llama3:8b] Output uses 97 tokens, remaining for response: 3808.
2025-08-15 01:22:29,889 INFO: JS Output: 355

2025-08-15 01:22:29,889 INFO: [MODEL: llama3:8b] Input token usage: 8.67%
2025-08-15 01:22:29,889 INFO: [MODEL: llama3:8b] Input uses 355 tokens, remaining for response: 3741.
2025-08-15 01:22:44,077 INFO: JS Output: 189

2025-08-15 01:22:44,077 INFO: [MODEL: llama3:8b] Output token usage: 4.61%
2025-08-15 01:22:44,078 INFO: [MODEL: llama3:8b] Output uses 189 tokens, remaining for response: 3552.
2025-08-15 01:22:44,557 INFO: JS Output: 191

2025-08-15 01:22:44,557 INFO: [MODEL: llama3:8b] Input token usage: 4.66%
2025-08-15 01:22:44,558 INFO: [MODEL: llama3:8b] Input uses 191 tokens, remaining for response: 3905.
2025-08-15 01:22:52,421 INFO: JS Output: 87

2025-08-15 01:22:52,422 INFO: [MODEL: llama3:8b] Output token usage: 2.12%
2025-08-15 01:22:52,422 INFO: [MODEL: llama3:8b] Output uses 87 tokens, remaining for response: 3818.
2025-08-15 01:22:52,887 INFO: JS Output: 194

2025-08-15 01:22:52,887 INFO: [MODEL: llama3:8b] Input token usage: 4.74%
2025-08-15 01:22:52,887 INFO: [MODEL: llama3:8b] Input uses 194 tokens, remaining for response: 3902.
2025-08-15 01:23:02,252 INFO: JS Output: 112

2025-08-15 01:23:02,253 INFO: [MODEL: llama3:8b] Output token usage: 2.73%
2025-08-15 01:23:02,253 INFO: [MODEL: llama3:8b] Output uses 112 tokens, remaining for response: 3790.
2025-08-15 01:23:02,728 INFO: JS Output: 326

2025-08-15 01:23:02,728 INFO: [MODEL: llama3:8b] Input token usage: 7.96%
2025-08-15 01:23:02,728 INFO: [MODEL: llama3:8b] Input uses 326 tokens, remaining for response: 3770.
2025-08-15 01:23:17,754 INFO: JS Output: 200

2025-08-15 01:23:17,754 INFO: [MODEL: llama3:8b] Output token usage: 4.88%
2025-08-15 01:23:17,755 INFO: [MODEL: llama3:8b] Output uses 200 tokens, remaining for response: 3570.
2025-08-15 01:23:18,243 INFO: JS Output: 194

2025-08-15 01:23:18,243 INFO: [MODEL: llama3:8b] Input token usage: 4.74%
2025-08-15 01:23:18,244 INFO: [MODEL: llama3:8b] Input uses 194 tokens, remaining for response: 3902.
2025-08-15 01:23:26,782 INFO: JS Output: 97

2025-08-15 01:23:26,783 INFO: [MODEL: llama3:8b] Output token usage: 2.37%
2025-08-15 01:23:26,783 INFO: [MODEL: llama3:8b] Output uses 97 tokens, remaining for response: 3805.
2025-08-15 01:23:27,264 INFO: JS Output: 350

2025-08-15 01:23:27,264 INFO: [MODEL: llama3:8b] Input token usage: 8.54%
2025-08-15 01:23:27,265 INFO: [MODEL: llama3:8b] Input uses 350 tokens, remaining for response: 3746.
2025-08-15 01:23:38,791 INFO: JS Output: 141

2025-08-15 01:23:38,791 INFO: [MODEL: llama3:8b] Output token usage: 3.44%
2025-08-15 01:23:38,792 INFO: [MODEL: llama3:8b] Output uses 141 tokens, remaining for response: 3605.
2025-08-15 01:23:39,288 INFO: JS Output: 369

2025-08-15 01:23:39,288 INFO: [MODEL: llama3:8b] Input token usage: 9.01%
2025-08-15 01:23:39,288 INFO: [MODEL: llama3:8b] Input uses 369 tokens, remaining for response: 3727.
2025-08-15 01:23:56,860 INFO: JS Output: 235

2025-08-15 01:23:56,860 INFO: [MODEL: llama3:8b] Output token usage: 5.74%
2025-08-15 01:23:56,860 INFO: [MODEL: llama3:8b] Output uses 235 tokens, remaining for response: 3492.
2025-08-15 01:23:57,332 INFO: JS Output: 350

2025-08-15 01:23:57,332 INFO: [MODEL: llama3:8b] Input token usage: 8.54%
2025-08-15 01:23:57,333 INFO: [MODEL: llama3:8b] Input uses 350 tokens, remaining for response: 3746.
2025-08-15 01:24:07,790 INFO: JS Output: 125

2025-08-15 01:24:07,791 INFO: [MODEL: llama3:8b] Output token usage: 3.05%
2025-08-15 01:24:07,791 INFO: [MODEL: llama3:8b] Output uses 125 tokens, remaining for response: 3621.
2025-08-15 01:24:08,252 INFO: JS Output: 269

2025-08-15 01:24:08,252 INFO: [MODEL: llama3:8b] Input token usage: 6.57%
2025-08-15 01:24:08,252 INFO: [MODEL: llama3:8b] Input uses 269 tokens, remaining for response: 3827.
2025-08-15 01:24:17,967 INFO: JS Output: 114

2025-08-15 01:24:17,968 INFO: [MODEL: llama3:8b] Output token usage: 2.78%
2025-08-15 01:24:17,968 INFO: [MODEL: llama3:8b] Output uses 114 tokens, remaining for response: 3713.
2025-08-15 01:24:18,435 INFO: JS Output: 367

2025-08-15 01:24:18,436 INFO: [MODEL: llama3:8b] Input token usage: 8.96%
2025-08-15 01:24:18,436 INFO: [MODEL: llama3:8b] Input uses 367 tokens, remaining for response: 3729.
2025-08-15 01:24:33,777 INFO: JS Output: 201

2025-08-15 01:24:33,777 INFO: [MODEL: llama3:8b] Output token usage: 4.91%
2025-08-15 01:24:33,777 INFO: [MODEL: llama3:8b] Output uses 201 tokens, remaining for response: 3528.
2025-08-15 01:24:34,255 INFO: JS Output: 269

2025-08-15 01:24:34,255 INFO: [MODEL: llama3:8b] Input token usage: 6.57%
2025-08-15 01:24:34,256 INFO: [MODEL: llama3:8b] Input uses 269 tokens, remaining for response: 3827.
2025-08-15 01:24:45,372 INFO: JS Output: 136

2025-08-15 01:24:45,372 INFO: [MODEL: llama3:8b] Output token usage: 3.32%
2025-08-15 01:24:45,372 INFO: [MODEL: llama3:8b] Output uses 136 tokens, remaining for response: 3691.
2025-08-15 01:24:45,873 INFO: JS Output: 187

2025-08-15 01:24:45,873 INFO: [MODEL: llama3:8b] Input token usage: 4.57%
2025-08-15 01:24:45,873 INFO: [MODEL: llama3:8b] Input uses 187 tokens, remaining for response: 3909.
2025-08-15 01:24:53,304 INFO: JS Output: 72

2025-08-15 01:24:53,304 INFO: [MODEL: llama3:8b] Output token usage: 1.76%
2025-08-15 01:24:53,304 INFO: [MODEL: llama3:8b] Output uses 72 tokens, remaining for response: 3837.
2025-08-15 01:24:53,826 INFO: JS Output: 330

2025-08-15 01:24:53,827 INFO: [MODEL: llama3:8b] Input token usage: 8.06%
2025-08-15 01:24:53,827 INFO: [MODEL: llama3:8b] Input uses 330 tokens, remaining for response: 3766.
2025-08-15 01:25:07,537 INFO: JS Output: 165

2025-08-15 01:25:07,537 INFO: [MODEL: llama3:8b] Output token usage: 4.03%
2025-08-15 01:25:07,537 INFO: [MODEL: llama3:8b] Output uses 165 tokens, remaining for response: 3601.
2025-08-15 01:25:08,034 INFO: JS Output: 184

2025-08-15 01:25:08,034 INFO: [MODEL: llama3:8b] Input token usage: 4.49%
2025-08-15 01:25:08,034 INFO: [MODEL: llama3:8b] Input uses 184 tokens, remaining for response: 3912.
2025-08-15 01:25:15,484 INFO: JS Output: 74

2025-08-15 01:25:15,484 INFO: [MODEL: llama3:8b] Output token usage: 1.81%
2025-08-15 01:25:15,485 INFO: [MODEL: llama3:8b] Output uses 74 tokens, remaining for response: 3838.
2025-08-15 01:25:15,966 INFO: JS Output: 72

2025-08-15 01:25:15,966 INFO: [MODEL: llama3:8b] Input token usage: 1.76%
2025-08-15 01:25:15,966 INFO: [MODEL: llama3:8b] Input uses 72 tokens, remaining for response: 4024.
2025-08-15 01:25:20,283 INFO: JS Output: 25

2025-08-15 01:25:20,283 INFO: [MODEL: llama3:8b] Output token usage: 0.61%
2025-08-15 01:25:20,283 INFO: [MODEL: llama3:8b] Output uses 25 tokens, remaining for response: 3999.
2025-08-15 01:25:20,790 INFO: JS Output: 935

2025-08-15 01:25:20,790 INFO: [MODEL: llama3:8b] Input token usage: 22.83%
2025-08-15 01:25:20,790 INFO: [MODEL: llama3:8b] Input uses 935 tokens, remaining for response: 3161.
2025-08-15 01:25:52,273 INFO: JS Output: 417

2025-08-15 01:25:52,274 INFO: [MODEL: llama3:8b] Output token usage: 10.18%
2025-08-15 01:25:52,274 INFO: [MODEL: llama3:8b] Output uses 417 tokens, remaining for response: 2744.
2025-08-15 01:25:52,747 INFO: JS Output: 566

2025-08-15 01:25:52,747 INFO: [MODEL: llama3:8b] Input token usage: 13.82%
2025-08-15 01:25:52,748 INFO: [MODEL: llama3:8b] Input uses 566 tokens, remaining for response: 3530.
2025-08-15 01:26:01,209 INFO: JS Output: 66

2025-08-15 01:26:01,209 INFO: [MODEL: llama3:8b] Output token usage: 1.61%
2025-08-15 01:26:01,209 INFO: [MODEL: llama3:8b] Output uses 66 tokens, remaining for response: 3464.
2025-08-15 01:26:01,210 INFO: [0]Consistency Checker Vs Job Description:

[1]Inconsistencies With Job Description: No; The resume accurately highlights Jane Doe's relevant skills and experiences, including her education, technical skills, and soft skills, which are all consistent with the job description.

[1]Suggestions for Improvement: None.
2025-08-15 01:26:01,211 INFO: [0]Consistency Checker Vs Job Description:
[1]Inconsistencies With Job Description: No; The resume accurately highlights Jane Doe's relevant skills and experiences, including her education, technical skills, and soft skills, which are all consistent with the job description.
[1]Suggestions for Improvement: None.
2025-08-15 01:26:01,211 INFO: Consistency Checker: Tailored Resume VS Original Resume:
2025-08-15 01:26:01,856 INFO: JS Output: 131

2025-08-15 01:26:01,856 INFO: [MODEL: llama3:8b] Input token usage: 3.20%
2025-08-15 01:26:01,856 INFO: [MODEL: llama3:8b] Input uses 131 tokens, remaining for response: 3965.
2025-08-15 01:26:12,004 INFO: JS Output: 104

2025-08-15 01:26:12,004 INFO: [MODEL: llama3:8b] Output token usage: 2.54%
2025-08-15 01:26:12,004 INFO: [MODEL: llama3:8b] Output uses 104 tokens, remaining for response: 3861.
2025-08-15 01:26:12,550 INFO: JS Output: 275

2025-08-15 01:26:12,551 INFO: [MODEL: llama3:8b] Input token usage: 6.71%
2025-08-15 01:26:12,551 INFO: [MODEL: llama3:8b] Input uses 275 tokens, remaining for response: 3821.
2025-08-15 01:26:23,122 INFO: JS Output: 107

2025-08-15 01:26:23,122 INFO: [MODEL: llama3:8b] Output token usage: 2.61%
2025-08-15 01:26:23,123 INFO: [MODEL: llama3:8b] Output uses 107 tokens, remaining for response: 3714.
2025-08-15 01:26:23,665 INFO: JS Output: 133

2025-08-15 01:26:23,665 INFO: [MODEL: llama3:8b] Input token usage: 3.25%
2025-08-15 01:26:23,665 INFO: [MODEL: llama3:8b] Input uses 133 tokens, remaining for response: 3963.
2025-08-15 01:26:36,167 INFO: JS Output: 138

2025-08-15 01:26:36,167 INFO: [MODEL: llama3:8b] Output token usage: 3.37%
2025-08-15 01:26:36,167 INFO: [MODEL: llama3:8b] Output uses 138 tokens, remaining for response: 3825.
2025-08-15 01:26:36,690 INFO: JS Output: 938

2025-08-15 01:26:36,690 INFO: [MODEL: llama3:8b] Input token usage: 22.90%
2025-08-15 01:26:36,690 INFO: [MODEL: llama3:8b] Input uses 938 tokens, remaining for response: 3158.
2025-08-15 01:26:59,068 INFO: JS Output: 251

2025-08-15 01:26:59,068 INFO: [MODEL: llama3:8b] Output token usage: 6.13%
2025-08-15 01:26:59,069 INFO: [MODEL: llama3:8b] Output uses 251 tokens, remaining for response: 2907.
2025-08-15 01:26:59,581 INFO: JS Output: 137

2025-08-15 01:26:59,581 INFO: [MODEL: llama3:8b] Input token usage: 3.34%
2025-08-15 01:26:59,582 INFO: [MODEL: llama3:8b] Input uses 137 tokens, remaining for response: 3959.
2025-08-15 01:27:07,793 INFO: JS Output: 78

2025-08-15 01:27:07,793 INFO: [MODEL: llama3:8b] Output token usage: 1.90%
2025-08-15 01:27:07,794 INFO: [MODEL: llama3:8b] Output uses 78 tokens, remaining for response: 3881.
2025-08-15 01:27:08,320 INFO: JS Output: 367

2025-08-15 01:27:08,321 INFO: [MODEL: llama3:8b] Input token usage: 8.96%
2025-08-15 01:27:08,321 INFO: [MODEL: llama3:8b] Input uses 367 tokens, remaining for response: 3729.
2025-08-15 01:27:25,198 INFO: JS Output: 192

2025-08-15 01:27:25,198 INFO: [MODEL: llama3:8b] Output token usage: 4.69%
2025-08-15 01:27:25,198 INFO: [MODEL: llama3:8b] Output uses 192 tokens, remaining for response: 3537.
2025-08-15 01:27:25,720 INFO: JS Output: 265

2025-08-15 01:27:25,721 INFO: [MODEL: llama3:8b] Input token usage: 6.47%
2025-08-15 01:27:25,721 INFO: [MODEL: llama3:8b] Input uses 265 tokens, remaining for response: 3831.
2025-08-15 01:27:39,028 INFO: JS Output: 147

2025-08-15 01:27:39,028 INFO: [MODEL: llama3:8b] Output token usage: 3.59%
2025-08-15 01:27:39,029 INFO: [MODEL: llama3:8b] Output uses 147 tokens, remaining for response: 3684.
2025-08-15 01:27:39,565 INFO: JS Output: 261

2025-08-15 01:27:39,565 INFO: [MODEL: llama3:8b] Input token usage: 6.37%
2025-08-15 01:27:39,566 INFO: [MODEL: llama3:8b] Input uses 261 tokens, remaining for response: 3835.
2025-08-15 01:27:56,609 INFO: JS Output: 196

2025-08-15 01:27:56,610 INFO: [MODEL: llama3:8b] Output token usage: 4.79%
2025-08-15 01:27:56,610 INFO: [MODEL: llama3:8b] Output uses 196 tokens, remaining for response: 3639.
2025-08-15 01:27:57,139 INFO: JS Output: 2290

2025-08-15 01:27:57,140 INFO: [MODEL: llama3:8b] Input token usage: 55.91%
2025-08-15 01:27:57,140 INFO: [MODEL: llama3:8b] Input uses 2290 tokens, remaining for response: 1806.
2025-08-15 01:28:23,512 INFO: JS Output: 247

2025-08-15 01:28:23,512 INFO: [MODEL: llama3:8b] Output token usage: 6.03%
2025-08-15 01:28:23,512 INFO: [MODEL: llama3:8b] Output uses 247 tokens, remaining for response: 1559.
2025-08-15 01:28:24,104 INFO: JS Output: 2065

2025-08-15 01:28:24,104 INFO: [MODEL: llama3:8b] Input token usage: 50.42%
2025-08-15 01:28:24,105 INFO: [MODEL: llama3:8b] Input uses 2065 tokens, remaining for response: 2031.
2025-08-15 01:28:47,147 INFO: JS Output: 216

2025-08-15 01:28:47,147 INFO: [MODEL: llama3:8b] Output token usage: 5.27%
2025-08-15 01:28:47,147 INFO: [MODEL: llama3:8b] Output uses 216 tokens, remaining for response: 1815.
2025-08-15 01:28:47,693 INFO: JS Output: 1700

2025-08-15 01:28:47,693 INFO: [MODEL: llama3:8b] Input token usage: 41.50%
2025-08-15 01:28:47,693 INFO: [MODEL: llama3:8b] Input uses 1700 tokens, remaining for response: 2396.
2025-08-15 01:29:11,447 INFO: JS Output: 238

2025-08-15 01:29:11,447 INFO: [MODEL: llama3:8b] Output token usage: 5.81%
2025-08-15 01:29:11,448 INFO: [MODEL: llama3:8b] Output uses 238 tokens, remaining for response: 2158.
2025-08-15 01:29:12,012 INFO: JS Output: 196

2025-08-15 01:29:12,013 INFO: [MODEL: llama3:8b] Input token usage: 4.79%
2025-08-15 01:29:12,013 INFO: [MODEL: llama3:8b] Input uses 196 tokens, remaining for response: 3900.
2025-08-15 01:29:27,461 INFO: JS Output: 175

2025-08-15 01:29:27,462 INFO: [MODEL: llama3:8b] Output token usage: 4.27%
2025-08-15 01:29:27,462 INFO: [MODEL: llama3:8b] Output uses 175 tokens, remaining for response: 3725.
2025-08-15 01:29:28,030 INFO: JS Output: 534

2025-08-15 01:29:28,031 INFO: [MODEL: llama3:8b] Input token usage: 13.04%
2025-08-15 01:29:28,031 INFO: [MODEL: llama3:8b] Input uses 534 tokens, remaining for response: 3562.
2025-08-15 01:29:38,800 INFO: JS Output: 100

2025-08-15 01:29:38,800 INFO: [MODEL: llama3:8b] Output token usage: 2.44%
2025-08-15 01:29:38,801 INFO: [MODEL: llama3:8b] Output uses 100 tokens, remaining for response: 3462.
2025-08-15 01:29:38,802 INFO: [0]Consistency Checker VS Original Resume:

[1]Inconsistencies With Original Resume: No; There are no inconsistencies with the original resume. All information presented in the new resume is present in the original resume, and there is no made-up information added.

[1]Inconsistencies With Self: No; The analysis shows that the tailored resume section accurately condenses the raw untailored resume section, and there are no contradictions or inconsistencies within the new resume.
2025-08-15 01:29:38,803 INFO: [0]Consistency Checker VS Original Resume:
[1]Inconsistencies With Original Resume: No; There are no inconsistencies with the original resume. All information presented in the new resume is present in the original resume, and there is no made-up information added.
[1]Inconsistencies With Self: No; The analysis shows that the tailored resume section accurately condenses the raw untailored resume section, and there are no contradictions or inconsistencies within the new resume.
2025-08-15 01:51:29,302 INFO: Tailoring cover letter with model: llama3:8b
2025-08-15 01:51:29,794 INFO: JS Output: 97

2025-08-15 01:51:29,795 INFO: [MODEL: llama3:8b] Input token usage: 2.37%
2025-08-15 01:51:29,795 INFO: [MODEL: llama3:8b] Input uses 97 tokens, remaining for response: 3999.
2025-08-15 01:51:57,580 INFO: JS Output: 327

2025-08-15 01:51:57,580 INFO: [MODEL: llama3:8b] Output token usage: 7.98%
2025-08-15 01:51:57,580 INFO: [MODEL: llama3:8b] Output uses 327 tokens, remaining for response: 3672.
2025-08-15 01:51:57,581 INFO: slide_summary: candidate_name: Jane Doe
2025-08-15 01:51:57,581 INFO: slide_summary: candidate_title: Senior Software Engineer
2025-08-15 01:51:57,581 INFO: slide_summary: general_txts: 4
2025-08-15 01:51:57,581 INFO: slide_summary: special_txts: 6
2025-08-15 01:51:58,123 INFO: JS Output: 241

2025-08-15 01:51:58,124 INFO: [MODEL: llama3:8b] Input token usage: 5.88%
2025-08-15 01:51:58,124 INFO: [MODEL: llama3:8b] Input uses 241 tokens, remaining for response: 3855.
2025-08-15 01:52:09,432 INFO: JS Output: 125

2025-08-15 01:52:09,432 INFO: [MODEL: llama3:8b] Output token usage: 3.05%
2025-08-15 01:52:09,432 INFO: [MODEL: llama3:8b] Output uses 125 tokens, remaining for response: 3730.
2025-08-15 01:52:09,992 INFO: JS Output: 191

2025-08-15 01:52:09,992 INFO: [MODEL: llama3:8b] Input token usage: 4.66%
2025-08-15 01:52:09,992 INFO: [MODEL: llama3:8b] Input uses 191 tokens, remaining for response: 3905.
2025-08-15 01:52:19,574 INFO: JS Output: 103

2025-08-15 01:52:19,574 INFO: [MODEL: llama3:8b] Output token usage: 2.51%
2025-08-15 01:52:19,575 INFO: [MODEL: llama3:8b] Output uses 103 tokens, remaining for response: 3802.
2025-08-15 01:52:20,066 INFO: JS Output: 349

2025-08-15 01:52:20,066 INFO: [MODEL: llama3:8b] Input token usage: 8.52%
2025-08-15 01:52:20,067 INFO: [MODEL: llama3:8b] Input uses 349 tokens, remaining for response: 3747.
2025-08-15 01:52:37,064 INFO: JS Output: 207

2025-08-15 01:52:37,064 INFO: [MODEL: llama3:8b] Output token usage: 5.05%
2025-08-15 01:52:37,064 INFO: [MODEL: llama3:8b] Output uses 207 tokens, remaining for response: 3540.
2025-08-15 01:52:37,568 INFO: JS Output: 191

2025-08-15 01:52:37,568 INFO: [MODEL: llama3:8b] Input token usage: 4.66%
2025-08-15 01:52:37,568 INFO: [MODEL: llama3:8b] Input uses 191 tokens, remaining for response: 3905.
2025-08-15 01:52:48,591 INFO: JS Output: 121

2025-08-15 01:52:48,591 INFO: [MODEL: llama3:8b] Output token usage: 2.95%
2025-08-15 01:52:48,591 INFO: [MODEL: llama3:8b] Output uses 121 tokens, remaining for response: 3784.
2025-08-15 01:52:49,097 INFO: JS Output: 194

2025-08-15 01:52:49,097 INFO: [MODEL: llama3:8b] Input token usage: 4.74%
2025-08-15 01:52:49,098 INFO: [MODEL: llama3:8b] Input uses 194 tokens, remaining for response: 3902.
2025-08-15 01:52:58,457 INFO: JS Output: 101

2025-08-15 01:52:58,457 INFO: [MODEL: llama3:8b] Output token usage: 2.47%
2025-08-15 01:52:58,458 INFO: [MODEL: llama3:8b] Output uses 101 tokens, remaining for response: 3801.
2025-08-15 01:52:58,955 INFO: JS Output: 349

2025-08-15 01:52:58,956 INFO: [MODEL: llama3:8b] Input token usage: 8.52%
2025-08-15 01:52:58,956 INFO: [MODEL: llama3:8b] Input uses 349 tokens, remaining for response: 3747.
2025-08-15 01:53:16,168 INFO: JS Output: 211

2025-08-15 01:53:16,168 INFO: [MODEL: llama3:8b] Output token usage: 5.15%
2025-08-15 01:53:16,169 INFO: [MODEL: llama3:8b] Output uses 211 tokens, remaining for response: 3536.
2025-08-15 01:53:16,677 INFO: JS Output: 194

2025-08-15 01:53:16,678 INFO: [MODEL: llama3:8b] Input token usage: 4.74%
2025-08-15 01:53:16,678 INFO: [MODEL: llama3:8b] Input uses 194 tokens, remaining for response: 3902.
2025-08-15 01:53:26,811 INFO: JS Output: 107

2025-08-15 01:53:26,811 INFO: [MODEL: llama3:8b] Output token usage: 2.61%
2025-08-15 01:53:26,811 INFO: [MODEL: llama3:8b] Output uses 107 tokens, remaining for response: 3795.
2025-08-15 01:53:27,316 INFO: JS Output: 350

2025-08-15 01:53:27,316 INFO: [MODEL: llama3:8b] Input token usage: 8.54%
2025-08-15 01:53:27,316 INFO: [MODEL: llama3:8b] Input uses 350 tokens, remaining for response: 3746.
2025-08-15 01:53:40,297 INFO: JS Output: 146

2025-08-15 01:53:40,298 INFO: [MODEL: llama3:8b] Output token usage: 3.56%
2025-08-15 01:53:40,298 INFO: [MODEL: llama3:8b] Output uses 146 tokens, remaining for response: 3600.
2025-08-15 01:53:40,815 INFO: JS Output: 384

2025-08-15 01:53:40,816 INFO: [MODEL: llama3:8b] Input token usage: 9.38%
2025-08-15 01:53:40,816 INFO: [MODEL: llama3:8b] Input uses 384 tokens, remaining for response: 3712.
2025-08-15 01:53:57,661 INFO: JS Output: 202

2025-08-15 01:53:57,661 INFO: [MODEL: llama3:8b] Output token usage: 4.93%
2025-08-15 01:53:57,662 INFO: [MODEL: llama3:8b] Output uses 202 tokens, remaining for response: 3510.
2025-08-15 01:53:58,187 INFO: JS Output: 350

2025-08-15 01:53:58,188 INFO: [MODEL: llama3:8b] Input token usage: 8.54%
2025-08-15 01:53:58,188 INFO: [MODEL: llama3:8b] Input uses 350 tokens, remaining for response: 3746.
2025-08-15 01:54:07,775 INFO: JS Output: 97

2025-08-15 01:54:07,775 INFO: [MODEL: llama3:8b] Output token usage: 2.37%
2025-08-15 01:54:07,775 INFO: [MODEL: llama3:8b] Output uses 97 tokens, remaining for response: 3649.
2025-08-15 01:54:08,281 INFO: JS Output: 269

2025-08-15 01:54:08,281 INFO: [MODEL: llama3:8b] Input token usage: 6.57%
2025-08-15 01:54:08,281 INFO: [MODEL: llama3:8b] Input uses 269 tokens, remaining for response: 3827.
2025-08-15 01:54:18,568 INFO: JS Output: 109

2025-08-15 01:54:18,569 INFO: [MODEL: llama3:8b] Output token usage: 2.66%
2025-08-15 01:54:18,569 INFO: [MODEL: llama3:8b] Output uses 109 tokens, remaining for response: 3718.
2025-08-15 01:54:19,097 INFO: JS Output: 334

2025-08-15 01:54:19,097 INFO: [MODEL: llama3:8b] Input token usage: 8.15%
2025-08-15 01:54:19,097 INFO: [MODEL: llama3:8b] Input uses 334 tokens, remaining for response: 3762.
2025-08-15 01:54:34,611 INFO: JS Output: 182

2025-08-15 01:54:34,613 INFO: [MODEL: llama3:8b] Output token usage: 4.44%
2025-08-15 01:54:34,613 INFO: [MODEL: llama3:8b] Output uses 182 tokens, remaining for response: 3580.
2025-08-15 01:54:35,143 INFO: JS Output: 269

2025-08-15 01:54:35,143 INFO: [MODEL: llama3:8b] Input token usage: 6.57%
2025-08-15 01:54:35,144 INFO: [MODEL: llama3:8b] Input uses 269 tokens, remaining for response: 3827.
2025-08-15 01:54:45,584 INFO: JS Output: 109

2025-08-15 01:54:45,584 INFO: [MODEL: llama3:8b] Output token usage: 2.66%
2025-08-15 01:54:45,585 INFO: [MODEL: llama3:8b] Output uses 109 tokens, remaining for response: 3718.
2025-08-15 01:54:46,099 INFO: JS Output: 187

2025-08-15 01:54:46,099 INFO: [MODEL: llama3:8b] Input token usage: 4.57%
2025-08-15 01:54:46,100 INFO: [MODEL: llama3:8b] Input uses 187 tokens, remaining for response: 3909.
2025-08-15 01:54:55,750 INFO: JS Output: 100

2025-08-15 01:54:55,751 INFO: [MODEL: llama3:8b] Output token usage: 2.44%
2025-08-15 01:54:55,751 INFO: [MODEL: llama3:8b] Output uses 100 tokens, remaining for response: 3809.
2025-08-15 01:54:56,275 INFO: JS Output: 331

2025-08-15 01:54:56,276 INFO: [MODEL: llama3:8b] Input token usage: 8.08%
2025-08-15 01:54:56,276 INFO: [MODEL: llama3:8b] Input uses 331 tokens, remaining for response: 3765.
2025-08-15 01:55:10,995 INFO: JS Output: 172

2025-08-15 01:55:10,996 INFO: [MODEL: llama3:8b] Output token usage: 4.20%
2025-08-15 01:55:10,996 INFO: [MODEL: llama3:8b] Output uses 172 tokens, remaining for response: 3593.
2025-08-15 01:55:11,525 INFO: JS Output: 184

2025-08-15 01:55:11,525 INFO: [MODEL: llama3:8b] Input token usage: 4.49%
2025-08-15 01:55:11,525 INFO: [MODEL: llama3:8b] Input uses 184 tokens, remaining for response: 3912.
2025-08-15 01:55:20,285 INFO: JS Output: 88

2025-08-15 01:55:20,285 INFO: [MODEL: llama3:8b] Output token usage: 2.15%
2025-08-15 01:55:20,286 INFO: [MODEL: llama3:8b] Output uses 88 tokens, remaining for response: 3824.
2025-08-15 01:55:20,784 INFO: JS Output: 72

2025-08-15 01:55:20,785 INFO: [MODEL: llama3:8b] Input token usage: 1.76%
2025-08-15 01:55:20,786 INFO: [MODEL: llama3:8b] Input uses 72 tokens, remaining for response: 4024.
2025-08-15 01:55:26,063 INFO: JS Output: 38

2025-08-15 01:55:26,065 INFO: [MODEL: llama3:8b] Output token usage: 0.93%
2025-08-15 01:55:26,065 INFO: [MODEL: llama3:8b] Output uses 38 tokens, remaining for response: 3986.
2025-08-15 01:55:26,572 INFO: JS Output: 924

2025-08-15 01:55:26,572 INFO: [MODEL: llama3:8b] Input token usage: 22.56%
2025-08-15 01:55:26,572 INFO: [MODEL: llama3:8b] Input uses 924 tokens, remaining for response: 3172.
2025-08-15 01:55:52,740 INFO: JS Output: 304

2025-08-15 01:55:52,740 INFO: [MODEL: llama3:8b] Output token usage: 7.42%
2025-08-15 01:55:52,740 INFO: [MODEL: llama3:8b] Output uses 304 tokens, remaining for response: 2868.
2025-08-15 01:55:53,275 INFO: JS Output: 898

2025-08-15 01:55:53,276 INFO: [MODEL: llama3:8b] Input token usage: 21.92%
2025-08-15 01:55:53,276 INFO: [MODEL: llama3:8b] Input uses 898 tokens, remaining for response: 3198.
2025-08-15 01:56:15,726 INFO: JS Output: 284

2025-08-15 01:56:15,727 INFO: [MODEL: llama3:8b] Output token usage: 6.93%
2025-08-15 01:56:15,727 INFO: [MODEL: llama3:8b] Output uses 284 tokens, remaining for response: 2914.
2025-08-15 01:56:15,727 INFO: Cover letter text generated successfully.
2025-08-15 01:56:16,204 INFO: JS Output: 97

2025-08-15 01:56:16,205 INFO: [MODEL: llama3:8b] Input token usage: 2.37%
2025-08-15 01:56:16,205 INFO: [MODEL: llama3:8b] Input uses 97 tokens, remaining for response: 3999.
2025-08-15 01:56:32,226 INFO: JS Output: 219

2025-08-15 01:56:32,226 INFO: [MODEL: llama3:8b] Output token usage: 5.35%
2025-08-15 01:56:32,227 INFO: [MODEL: llama3:8b] Output uses 219 tokens, remaining for response: 3780.
2025-08-15 01:56:32,695 INFO: JS Output: 888

2025-08-15 01:56:32,695 INFO: [MODEL: llama3:8b] Input token usage: 21.68%
2025-08-15 01:56:32,696 INFO: [MODEL: llama3:8b] Input uses 888 tokens, remaining for response: 3208.
2025-08-15 01:56:44,344 INFO: JS Output: 130

2025-08-15 01:56:44,345 INFO: [MODEL: llama3:8b] Output token usage: 3.17%
2025-08-15 01:56:44,345 INFO: [MODEL: llama3:8b] Output uses 130 tokens, remaining for response: 3078.
2025-08-15 01:56:44,345 INFO: [0]Consistency Checker Vs Job Description:
[1]Inconsistencies With Job Description: No; The cover letter's skills and experiences align perfectly with the job description, mentioning relevant technical skills like cloud computing, distributed systems, advanced programming, algorithms, data structures, operating systems, and databases. The cover letter also highlights the importance of scalability, performance optimization, and team collaboration, which are all mentioned in the job description as key requirements.

[1]Suggestions for Improvement: None; The tailored cover letter appears to be a strong match for the Backend Engineer role at DoorDash, with no obvious inconsistencies or areas for improvement.
2025-08-15 01:56:44,345 INFO: [0]Consistency Checker Vs Job Description:
[1]Inconsistencies With Job Description: No; The cover letter's skills and experiences align perfectly with the job description, mentioning relevant technical skills like cloud computing, distributed systems, advanced programming, algorithms, data structures, operating systems, and databases. The cover letter also highlights the importance of scalability, performance optimization, and team collaboration, which are all mentioned in the job description as key requirements.
[1]Suggestions for Improvement: None; The tailored cover letter appears to be a strong match for the Backend Engineer role at DoorDash, with no obvious inconsistencies or areas for improvement.
2025-08-15 01:56:44,346 INFO: Consistency Checker: Cover Letter VS Original Resume:
2025-08-15 01:56:44,346 INFO: slide_summary: candidate_name: Jane Doe
2025-08-15 01:56:44,346 INFO: slide_summary: candidate_title: Senior Software Engineer
2025-08-15 01:56:44,347 INFO: slide_summary: general_txts: 4
2025-08-15 01:56:44,347 INFO: slide_summary: special_txts: 6
2025-08-15 01:56:44,850 INFO: JS Output: 241

2025-08-15 01:56:44,850 INFO: [MODEL: llama3:8b] Input token usage: 5.88%
2025-08-15 01:56:44,850 INFO: [MODEL: llama3:8b] Input uses 241 tokens, remaining for response: 3855.
2025-08-15 01:56:56,086 INFO: JS Output: 133

2025-08-15 01:56:56,086 INFO: [MODEL: llama3:8b] Output token usage: 3.25%
2025-08-15 01:56:56,086 INFO: [MODEL: llama3:8b] Output uses 133 tokens, remaining for response: 3722.
2025-08-15 01:56:56,573 INFO: JS Output: 191

2025-08-15 01:56:56,573 INFO: [MODEL: llama3:8b] Input token usage: 4.66%
2025-08-15 01:56:56,573 INFO: [MODEL: llama3:8b] Input uses 191 tokens, remaining for response: 3905.
2025-08-15 01:57:05,478 INFO: JS Output: 104

2025-08-15 01:57:05,478 INFO: [MODEL: llama3:8b] Output token usage: 2.54%
2025-08-15 01:57:05,479 INFO: [MODEL: llama3:8b] Output uses 104 tokens, remaining for response: 3801.
2025-08-15 01:57:05,937 INFO: JS Output: 358

2025-08-15 01:57:05,937 INFO: [MODEL: llama3:8b] Input token usage: 8.74%
2025-08-15 01:57:05,937 INFO: [MODEL: llama3:8b] Input uses 358 tokens, remaining for response: 3738.
2025-08-15 01:57:22,414 INFO: JS Output: 199

2025-08-15 01:57:22,415 INFO: [MODEL: llama3:8b] Output token usage: 4.86%
2025-08-15 01:57:22,415 INFO: [MODEL: llama3:8b] Output uses 199 tokens, remaining for response: 3539.
2025-08-15 01:57:22,901 INFO: JS Output: 191

2025-08-15 01:57:22,902 INFO: [MODEL: llama3:8b] Input token usage: 4.66%
2025-08-15 01:57:22,902 INFO: [MODEL: llama3:8b] Input uses 191 tokens, remaining for response: 3905.
2025-08-15 01:57:33,298 INFO: JS Output: 117

2025-08-15 01:57:33,299 INFO: [MODEL: llama3:8b] Output token usage: 2.86%
2025-08-15 01:57:33,299 INFO: [MODEL: llama3:8b] Output uses 117 tokens, remaining for response: 3788.
2025-08-15 01:57:33,826 INFO: JS Output: 194

2025-08-15 01:57:33,826 INFO: [MODEL: llama3:8b] Input token usage: 4.74%
2025-08-15 01:57:33,827 INFO: [MODEL: llama3:8b] Input uses 194 tokens, remaining for response: 3902.
2025-08-15 01:57:43,637 INFO: JS Output: 108

2025-08-15 01:57:43,637 INFO: [MODEL: llama3:8b] Output token usage: 2.64%
2025-08-15 01:57:43,637 INFO: [MODEL: llama3:8b] Output uses 108 tokens, remaining for response: 3794.
2025-08-15 01:57:44,118 INFO: JS Output: 352

2025-08-15 01:57:44,118 INFO: [MODEL: llama3:8b] Input token usage: 8.59%
2025-08-15 01:57:44,119 INFO: [MODEL: llama3:8b] Input uses 352 tokens, remaining for response: 3744.
2025-08-15 01:57:59,232 INFO: JS Output: 175

2025-08-15 01:57:59,232 INFO: [MODEL: llama3:8b] Output token usage: 4.27%
2025-08-15 01:57:59,232 INFO: [MODEL: llama3:8b] Output uses 175 tokens, remaining for response: 3569.
2025-08-15 01:57:59,714 INFO: JS Output: 194

2025-08-15 01:57:59,714 INFO: [MODEL: llama3:8b] Input token usage: 4.74%
2025-08-15 01:57:59,714 INFO: [MODEL: llama3:8b] Input uses 194 tokens, remaining for response: 3902.
2025-08-15 01:58:08,882 INFO: JS Output: 99

2025-08-15 01:58:08,882 INFO: [MODEL: llama3:8b] Output token usage: 2.42%
2025-08-15 01:58:08,882 INFO: [MODEL: llama3:8b] Output uses 99 tokens, remaining for response: 3803.
2025-08-15 01:58:09,396 INFO: JS Output: 350

2025-08-15 01:58:09,397 INFO: [MODEL: llama3:8b] Input token usage: 8.54%
2025-08-15 01:58:09,397 INFO: [MODEL: llama3:8b] Input uses 350 tokens, remaining for response: 3746.
2025-08-15 01:58:21,084 INFO: JS Output: 138

2025-08-15 01:58:21,084 INFO: [MODEL: llama3:8b] Output token usage: 3.37%
2025-08-15 01:58:21,084 INFO: [MODEL: llama3:8b] Output uses 138 tokens, remaining for response: 3608.
2025-08-15 01:58:21,574 INFO: JS Output: 368

2025-08-15 01:58:21,575 INFO: [MODEL: llama3:8b] Input token usage: 8.98%
2025-08-15 01:58:21,575 INFO: [MODEL: llama3:8b] Input uses 368 tokens, remaining for response: 3728.
2025-08-15 01:58:39,090 INFO: JS Output: 224

2025-08-15 01:58:39,091 INFO: [MODEL: llama3:8b] Output token usage: 5.47%
2025-08-15 01:58:39,091 INFO: [MODEL: llama3:8b] Output uses 224 tokens, remaining for response: 3504.
2025-08-15 01:58:39,596 INFO: JS Output: 350

2025-08-15 01:58:39,596 INFO: [MODEL: llama3:8b] Input token usage: 8.54%
2025-08-15 01:58:39,596 INFO: [MODEL: llama3:8b] Input uses 350 tokens, remaining for response: 3746.
2025-08-15 01:58:49,650 INFO: JS Output: 113

2025-08-15 01:58:49,650 INFO: [MODEL: llama3:8b] Output token usage: 2.76%
2025-08-15 01:58:49,650 INFO: [MODEL: llama3:8b] Output uses 113 tokens, remaining for response: 3633.
2025-08-15 01:58:50,163 INFO: JS Output: 269

2025-08-15 01:58:50,163 INFO: [MODEL: llama3:8b] Input token usage: 6.57%
2025-08-15 01:58:50,163 INFO: [MODEL: llama3:8b] Input uses 269 tokens, remaining for response: 3827.
2025-08-15 01:59:01,176 INFO: JS Output: 130

2025-08-15 01:59:01,176 INFO: [MODEL: llama3:8b] Output token usage: 3.17%
2025-08-15 01:59:01,177 INFO: [MODEL: llama3:8b] Output uses 130 tokens, remaining for response: 3697.
2025-08-15 01:59:01,659 INFO: JS Output: 371

2025-08-15 01:59:01,659 INFO: [MODEL: llama3:8b] Input token usage: 9.06%
2025-08-15 01:59:01,659 INFO: [MODEL: llama3:8b] Input uses 371 tokens, remaining for response: 3725.
2025-08-15 01:59:17,412 INFO: JS Output: 203

2025-08-15 01:59:17,412 INFO: [MODEL: llama3:8b] Output token usage: 4.96%
2025-08-15 01:59:17,413 INFO: [MODEL: llama3:8b] Output uses 203 tokens, remaining for response: 3522.
2025-08-15 01:59:17,904 INFO: JS Output: 269

2025-08-15 01:59:17,905 INFO: [MODEL: llama3:8b] Input token usage: 6.57%
2025-08-15 01:59:17,905 INFO: [MODEL: llama3:8b] Input uses 269 tokens, remaining for response: 3827.
2025-08-15 01:59:26,182 INFO: JS Output: 87

2025-08-15 01:59:26,182 INFO: [MODEL: llama3:8b] Output token usage: 2.12%
2025-08-15 01:59:26,182 INFO: [MODEL: llama3:8b] Output uses 87 tokens, remaining for response: 3740.
2025-08-15 01:59:26,713 INFO: JS Output: 187

2025-08-15 01:59:26,713 INFO: [MODEL: llama3:8b] Input token usage: 4.57%
2025-08-15 01:59:26,713 INFO: [MODEL: llama3:8b] Input uses 187 tokens, remaining for response: 3909.
2025-08-15 01:59:33,802 INFO: JS Output: 71

2025-08-15 01:59:33,803 INFO: [MODEL: llama3:8b] Output token usage: 1.73%
2025-08-15 01:59:33,803 INFO: [MODEL: llama3:8b] Output uses 71 tokens, remaining for response: 3838.
2025-08-15 01:59:34,307 INFO: JS Output: 280

2025-08-15 01:59:34,307 INFO: [MODEL: llama3:8b] Input token usage: 6.84%
2025-08-15 01:59:34,307 INFO: [MODEL: llama3:8b] Input uses 280 tokens, remaining for response: 3816.
2025-08-15 01:59:48,188 INFO: JS Output: 156

2025-08-15 01:59:48,188 INFO: [MODEL: llama3:8b] Output token usage: 3.81%
2025-08-15 01:59:48,188 INFO: [MODEL: llama3:8b] Output uses 156 tokens, remaining for response: 3660.
2025-08-15 01:59:48,692 INFO: JS Output: 184

2025-08-15 01:59:48,692 INFO: [MODEL: llama3:8b] Input token usage: 4.49%
2025-08-15 01:59:48,692 INFO: [MODEL: llama3:8b] Input uses 184 tokens, remaining for response: 3912.
2025-08-15 01:59:56,646 INFO: JS Output: 73

2025-08-15 01:59:56,646 INFO: [MODEL: llama3:8b] Output token usage: 1.78%
2025-08-15 01:59:56,646 INFO: [MODEL: llama3:8b] Output uses 73 tokens, remaining for response: 3839.
2025-08-15 01:59:57,127 INFO: JS Output: 72

2025-08-15 01:59:57,128 INFO: [MODEL: llama3:8b] Input token usage: 1.76%
2025-08-15 01:59:57,128 INFO: [MODEL: llama3:8b] Input uses 72 tokens, remaining for response: 4024.
2025-08-15 02:00:03,634 INFO: JS Output: 57

2025-08-15 02:00:03,634 INFO: [MODEL: llama3:8b] Output token usage: 1.39%
2025-08-15 02:00:03,635 INFO: [MODEL: llama3:8b] Output uses 57 tokens, remaining for response: 3967.
2025-08-15 02:00:04,137 INFO: JS Output: 723

2025-08-15 02:00:04,137 INFO: [MODEL: llama3:8b] Input token usage: 17.65%
2025-08-15 02:00:04,137 INFO: [MODEL: llama3:8b] Input uses 723 tokens, remaining for response: 3373.
2025-08-15 02:00:26,019 INFO: JS Output: 265

2025-08-15 02:00:26,020 INFO: [MODEL: llama3:8b] Output token usage: 6.47%
2025-08-15 02:00:26,020 INFO: [MODEL: llama3:8b] Output uses 265 tokens, remaining for response: 3108.
2025-08-15 02:00:26,540 INFO: JS Output: 1013

2025-08-15 02:00:26,541 INFO: [MODEL: llama3:8b] Input token usage: 24.73%
2025-08-15 02:00:26,541 INFO: [MODEL: llama3:8b] Input uses 1013 tokens, remaining for response: 3083.
2025-08-15 02:00:44,133 INFO: JS Output: 194

2025-08-15 02:00:44,134 INFO: [MODEL: llama3:8b] Output token usage: 4.74%
2025-08-15 02:00:44,134 INFO: [MODEL: llama3:8b] Output uses 194 tokens, remaining for response: 2889.
2025-08-15 02:00:44,135 INFO: [0]Consistency Checker Vs Resume:
[1]Inconsistencies With Resume: No; The cover letter accurately mentions skills and experiences that are also present in the resume, such as cloud computing, distributed systems, advanced programming, algorithms, data structures, operating systems, databases, Java, Python, C++, Go, microservices architecture, containerization using Docker, scalability and performance optimization techniques, team collaboration, problem-solving, and experience working with engineers, product managers, and stakeholders. There are no inconsistencies found between the cover letter and the resume.
[1]Inconsistencies With Self: None; The cover letter does not contain any contradictions or inconsistencies in the information provided. Each paragraph logically flows from one to another, and there is no contradictory statement made throughout the letter.
[1]Suggestions for Improvement: None; The tailored cover letter appears to be well-written and accurately reflects Jane Doe's skills and experiences as a Senior Software Engineer.
2025-08-15 02:00:44,135 INFO: [0]Consistency Checker Vs Resume:
[1]Inconsistencies With Resume: No; The cover letter accurately mentions skills and experiences that are also present in the resume, such as cloud computing, distributed systems, advanced programming, algorithms, data structures, operating systems, databases, Java, Python, C++, Go, microservices architecture, containerization using Docker, scalability and performance optimization techniques, team collaboration, problem-solving, and experience working with engineers, product managers, and stakeholders. There are no inconsistencies found between the cover letter and the resume.
[1]Inconsistencies With Self: None; The cover letter does not contain any contradictions or inconsistencies in the information provided. Each paragraph logically flows from one to another, and there is no contradictory statement made throughout the letter.
[1]Suggestions for Improvement: None; The tailored cover letter appears to be well-written and accurately reflects Jane Doe's skills and experiences as a Senior Software Engineer.
2025-08-15 02:01:56,042 INFO: Cover letter saved to C:\CodeProjects\Sisyphus\Sisyphus\saved_outputs_cl\cltest0.txt
